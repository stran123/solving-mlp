{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving Machine Learning Problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece==0.1.91\n",
      "  Using cached sentencepiece-0.1.91-cp38-cp38-manylinux1_x86_64.whl (1.1 MB)\n",
      "Installing collected packages: sentencepiece\n",
      "  Attempting uninstall: sentencepiece\n",
      "    Found existing installation: sentencepiece 0.1.91\n",
      "    Uninstalling sentencepiece-0.1.91:\n",
      "      Successfully uninstalled sentencepiece-0.1.91\n",
      "Successfully installed sentencepiece-0.1.91\n",
      "Requirement already satisfied: transformers in /home/sunnyt/anaconda3/lib/python3.8/site-packages (4.4.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/sunnyt/anaconda3/lib/python3.8/site-packages (from transformers) (2020.10.15)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/sunnyt/anaconda3/lib/python3.8/site-packages (from transformers) (0.10.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/sunnyt/anaconda3/lib/python3.8/site-packages (from transformers) (4.50.2)\n",
      "Requirement already satisfied: filelock in /home/sunnyt/anaconda3/lib/python3.8/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: sacremoses in /home/sunnyt/anaconda3/lib/python3.8/site-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: requests in /home/sunnyt/anaconda3/lib/python3.8/site-packages (from transformers) (2.24.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/sunnyt/anaconda3/lib/python3.8/site-packages (from transformers) (1.19.2)\n",
      "Requirement already satisfied: packaging in /home/sunnyt/anaconda3/lib/python3.8/site-packages (from transformers) (20.4)\n",
      "Requirement already satisfied: click in /home/sunnyt/anaconda3/lib/python3.8/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /home/sunnyt/anaconda3/lib/python3.8/site-packages (from sacremoses->transformers) (0.17.0)\n",
      "Requirement already satisfied: six in /home/sunnyt/anaconda3/lib/python3.8/site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/sunnyt/anaconda3/lib/python3.8/site-packages (from requests->transformers) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/sunnyt/anaconda3/lib/python3.8/site-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/sunnyt/anaconda3/lib/python3.8/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/sunnyt/anaconda3/lib/python3.8/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/sunnyt/anaconda3/lib/python3.8/site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: dgl in /home/sunnyt/anaconda3/lib/python3.8/site-packages (0.6.0.post1)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /home/sunnyt/anaconda3/lib/python3.8/site-packages (from dgl) (1.5.2)\n",
      "Requirement already satisfied: networkx>=2.1 in /home/sunnyt/anaconda3/lib/python3.8/site-packages (from dgl) (2.5)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /home/sunnyt/anaconda3/lib/python3.8/site-packages (from dgl) (1.19.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/sunnyt/anaconda3/lib/python3.8/site-packages (from dgl) (2.24.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /home/sunnyt/anaconda3/lib/python3.8/site-packages (from networkx>=2.1->dgl) (4.4.2)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/sunnyt/anaconda3/lib/python3.8/site-packages (from requests>=2.19.0->dgl) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/sunnyt/anaconda3/lib/python3.8/site-packages (from requests>=2.19.0->dgl) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/sunnyt/anaconda3/lib/python3.8/site-packages (from requests>=2.19.0->dgl) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/sunnyt/anaconda3/lib/python3.8/site-packages (from requests>=2.19.0->dgl) (2020.6.20)\n",
      "Requirement already satisfied: torchvision in /home/sunnyt/anaconda3/lib/python3.8/site-packages (0.9.1)\n",
      "Requirement already satisfied: numpy in /home/sunnyt/anaconda3/lib/python3.8/site-packages (from torchvision) (1.19.2)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/sunnyt/anaconda3/lib/python3.8/site-packages (from torchvision) (8.0.1)\n",
      "Requirement already satisfied: torch==1.8.1 in /home/sunnyt/anaconda3/lib/python3.8/site-packages (from torchvision) (1.8.1)\n",
      "Requirement already satisfied: typing-extensions in /home/sunnyt/anaconda3/lib/python3.8/site-packages (from torch==1.8.1->torchvision) (3.7.4.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install 'sentencepiece==0.1.91' --force-reinstall\n",
    "!pip install transformers\n",
    "!pip install dgl\n",
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-29T08:38:46.536166Z",
     "start_time": "2020-10-29T08:38:37.253954Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "## Needs: pytorch, dgl, transformers, Python>=3.7\n",
    "\n",
    "from copy import copy\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import itertools\n",
    "import importlib\n",
    "\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle\n",
    "from interruptingcow import timeout, Quota\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import dgl\n",
    "from dgl.nn import GraphConv\n",
    "false = False\n",
    "true = True\n",
    "NaN = float(\"NaN\")\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "import util\n",
    "importlib.reload(util)\n",
    "from util import setup, check_match, evaluate_prefix_expression, sub_nP, get_quant_cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting Inputs to Torch Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-29T08:30:17.523815Z",
     "start_time": "2020-10-29T08:30:17.503632Z"
    }
   },
   "outputs": [],
   "source": [
    "def tensorize_data(train_data, test_data):\n",
    "#     print(f\"Number of items: {len(train_data)+len(test_data)}\")\n",
    "    for d in tqdm(itertools.chain(train_data, test_data)):\n",
    "        d['in_idxs'] = torch.tensor([in_vocab.token2idx.get(x, in_vocab.unk) for x in d['in_tokens']])\n",
    "        d['out_idxs'] = torch.tensor([out_vocab.token2idx.get(x, out_vocab.unk) for x in d['out_tokens']])\n",
    "        d['n_in'] = n_in = len(d['in_idxs'])\n",
    "        d['n_out'] = len(d['out_idxs'])\n",
    "        d['n_nP'] = n_nP = len(d['nP'])\n",
    "        d['nP_in_mask'] = mask = torch.zeros(n_in, dtype=torch.bool)\n",
    "        mask[d['nP_positions']] = True\n",
    "        d['nP_out_mask'] = mask = torch.zeros(n_max_nP, dtype=torch.bool)\n",
    "        mask[:n_nP] = True\n",
    "        d['qcomp_edges'] = get_quantity_comparison_edges(d)\n",
    "        d['qcell_edges'] = get_quantity_cell_edges(d)\n",
    "\n",
    "def get_quantity_comparison_edges(d):\n",
    "    quants = [float(x) for x in d['nP']]\n",
    "    quant_positions = d['nP_positions']\n",
    "#     assert max(quant_positions) < d['n_in']\n",
    "    adj_matrix = torch.eye(d['n_in'], dtype=np.bool)\n",
    "    for x, x_pos in zip(quants, quant_positions):\n",
    "        for y, y_pos in zip(quants, quant_positions):\n",
    "            adj_matrix[x_pos, y_pos] |= x > y\n",
    "    \"\"\"\n",
    "    Convert the adjacency matrix of the directed graph into a tuple of (src_edges, dst_edges), which\n",
    "    is the input format of dgl.graph (see https://docs.dgl.ai/generated/dgl.graph.html).\n",
    "    Hint: check out the 'nonzero' function\n",
    "    \"\"\"\n",
    "    return adj_matrix.nonzero(as_tuple=True)\n",
    "\n",
    "def get_quantity_cell_edges(d):\n",
    "    in_idxs = d['in_idxs']\n",
    "    quant_positions = d['nP_positions']\n",
    "    quant_cell_positions = d['quant_cell_positions']\n",
    "    assert max(quant_cell_positions) < d['n_in']\n",
    "    word_cells = set(quant_cell_positions) - set(quant_positions)\n",
    "    adj_matrix = torch.eye(d['n_in'], dtype=torch.bool)\n",
    "    for w_pos in word_cells:\n",
    "        for q_pos in quant_positions:\n",
    "            if abs(w_pos - q_pos) < 4:\n",
    "                adj_matrix[w_pos, q_pos] = adj_matrix[q_pos, w_pos] = True\n",
    "    pos_idxs = in_idxs[quant_cell_positions]\n",
    "    for idx1, pos1 in zip(pos_idxs, quant_cell_positions):\n",
    "        for idx2, pos2 in zip(pos_idxs, quant_cell_positions):\n",
    "            if idx1 == idx2:\n",
    "                adj_matrix[pos1, pos2] = adj_matrix[pos2, pos1] = True\n",
    "    \"\"\"\n",
    "    Convert the adjacency matrix of the directed graph into a tuple of (src_edges, dst_edges), which\n",
    "    is the input format of dgl.graph (see https://docs.dgl.ai/generated/dgl.graph.html).\n",
    "    Hint: check out the 'nonzero' function\n",
    "    \"\"\"\n",
    "    return adj_matrix.nonzero(as_tuple=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     102
    ]
   },
   "outputs": [],
   "source": [
    "class TransformerAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Used in Transformer Block\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.qkv = nn.Linear(n_hid, n_head * (n_k * 2 + n_v))\n",
    "        self.out = nn.Linear(n_head * n_v, n_hid)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        n_batch, n_batch_max_in, n_hid = x.shape\n",
    "        q_k_v = self.qkv(x).view(n_batch, n_batch_max_in, n_head, 2 * n_k + n_v).transpose(1, 2)\n",
    "        q, k, v = q_k_v.split([n_k, n_k, n_v], dim=-1)\n",
    "\n",
    "        q = q.reshape(n_batch * n_head, n_batch_max_in, n_k)\n",
    "        k = k.reshape_as(q).transpose(1, 2)\n",
    "        qk = q.bmm(k) / np.sqrt(n_k)\n",
    "\n",
    "        if mask is not None:\n",
    "            qk = qk.view(n_batch, n_head, n_batch_max_in, n_batch_max_in).transpose(1, 2)\n",
    "            qk[~mask] = -np.inf\n",
    "            qk = qk.transpose(1, 2).view(n_batch * n_head, n_batch_max_in, n_batch_max_in)\n",
    "        qk = qk.softmax(dim=-1)\n",
    "        v = v.reshape(n_batch * n_head, n_batch_max_in, n_v)\n",
    "        qkv = qk.bmm(v).view(n_batch, n_head, n_batch_max_in, n_v).transpose(1, 2).reshape(n_batch, n_batch_max_in, n_head * n_v)\n",
    "        out = self.out(qkv)\n",
    "        return x + out\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom Transformer\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.attn = TransformerAttention()\n",
    "        n_inner = n_hid * 4\n",
    "        self.inner = nn.Sequential(\n",
    "            nn.Linear(n_hid, n_inner),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(n_inner, n_hid)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = x + self.attn(x, mask=mask)\n",
    "        return x + self.inner(x)\n",
    "    \n",
    "class GCNBranch(nn.Module):\n",
    "    def __init__(self, n_hid_in, n_hid_out, dropout=0.3):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Define a branch of the graph convolution with\n",
    "        1. GraphConv from n_hid_in to n_hid_in\n",
    "        2. ReLU\n",
    "        3. Dropout\n",
    "        4. GraphConv from n_hid_in to n_hid_out\n",
    "        \n",
    "        Note: your should call GraphConv with allow_zero_in_degree=True\n",
    "        \"\"\"\n",
    "        self.gc1 = GraphConv(n_hid_in, n_hid_in, allow_zero_in_degree=True)\n",
    "        self.drelu = nn.Sequential(\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.gc2 = GraphConv(n_hid_in, n_hid_out, allow_zero_in_degree=True)\n",
    "\n",
    "    def forward(self, x, graph):\n",
    "        \"\"\"\n",
    "        Forward pass of your defined branch above\n",
    "        \"\"\"\n",
    "        return self.gc2(graph, self.drelu(self.gc1(graph, x)))\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, n_head=4, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.branches = nn.ModuleList(GCNBranch(n_hid, n_hid // n_head, dropout) for _ in range(n_head))\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(n_hid, n_hid),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(n_hid, n_hid)\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(n_hid)\n",
    "\n",
    "    def forward(self, h, gt_graph, attr_graph):\n",
    "        x = h.reshape(-1, n_hid)\n",
    "        graphs = [gt_graph, gt_graph, attr_graph, attr_graph]\n",
    "        x = torch.cat([branch(x, g) for branch, g in zip(self.branches, graphs)], dim=-1).view_as(h)\n",
    "        x = h + self.layer_norm(x)\n",
    "        return x + self.feed_forward(x)\n",
    "\n",
    "class Gate(nn.Module):\n",
    "    def __init__(self, n_in, n_out):\n",
    "        super(Gate, self).__init__()\n",
    "        self.t = nn.Linear(n_in, n_out)\n",
    "        self.s = nn.Linear(n_in, n_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.t(x).tanh() * self.s(x).sigmoid()\n",
    "\n",
    "class TreeDecoder(nn.Module):\n",
    "    def __init__(self, dropout=0.5):\n",
    "        super().__init__()\n",
    "        drop = nn.Dropout(dropout)\n",
    "        self.constant_embedding = nn.Parameter(torch.randn(1, out_vocab.n_constants, n_hid))\n",
    "\n",
    "        self.qp_gate = nn.Sequential(drop, Gate(n_hid, n_hid))\n",
    "        self.gts_right = nn.Sequential(drop, Gate(2 * n_hid, n_hid))\n",
    "\n",
    "        self.attn_fc = nn.Sequential(drop,\n",
    "            nn.Linear(2 * n_hid, n_hid),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(n_hid, 1)\n",
    "        )\n",
    "        self.quant_fc = nn.Sequential(drop,\n",
    "            nn.Linear(n_hid * 3, n_hid),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(n_hid, 1, bias=False)\n",
    "        )\n",
    "        self.op_fc = nn.Sequential(drop, nn.Linear(n_hid * 2, out_vocab.n_ops))\n",
    "\n",
    "        self.op_embedding = nn.Embedding(out_vocab.n_ops + 1, n_hid, padding_idx=out_vocab.n_ops)\n",
    "        self.gts_left = nn.Sequential(drop, Gate(n_hid * 2 + n_hid, n_hid))\n",
    "        self.gts_left_qp = nn.Sequential(drop, Gate(n_hid * 2 + n_hid, n_hid), self.qp_gate)\n",
    "\n",
    "        self.subtree_gate = nn.Sequential(drop, Gate(n_hid * 2 + n_hid, n_hid))\n",
    "\n",
    "    def gts_attention(self, q, zbar, in_mask=None):\n",
    "        attn_score = self.attn_fc(\n",
    "            torch.cat([q.unsqueeze(1).expand_as(zbar), zbar], dim=2)\n",
    "        ).squeeze(2)\n",
    "        if in_mask is not None:\n",
    "            attn_score[~in_mask] = -np.inf\n",
    "        attn = attn_score.softmax(dim=1)\n",
    "        return (attn.unsqueeze(1) @ zbar).squeeze(1) # (n_batch, n_hid)\n",
    "\n",
    "    def gts_predict(self, qp_Gc, quant_embed, nP_out_mask=None):\n",
    "        quant_score = self.quant_fc(\n",
    "            torch.cat([qp_Gc.unsqueeze(1).expand(-1, quant_embed.size(1), -1), quant_embed], dim=2)\n",
    "        ).squeeze(2)\n",
    "        op_score = self.op_fc(qp_Gc)\n",
    "        pred_score = torch.cat((op_score, quant_score), dim=1)\n",
    "        if nP_out_mask is not None:\n",
    "            pred_score[:, out_vocab.base_nP:][~nP_out_mask] = -np.inf\n",
    "        return pred_score\n",
    "\n",
    "    def merge_subtree(self, op, tl, yr):\n",
    "        return self.subtree_gate(torch.cat((op, tl, yr), dim=-1))\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, dropout=0.5):\n",
    "        super().__init__()\n",
    "        drop = nn.Dropout(dropout)\n",
    "\n",
    "        if use_t5:\n",
    "            \"\"\"\n",
    "            Use t5_model.encoder as the encoder for this model. Note that unlike the custom transformer, you don't\n",
    "            need to use an external positional embedding for the T5 transformer (i.e. don't define self.pos_emb)\n",
    "            \n",
    "            You may specify layer weights to freeze during finetuning by modifying the freeze_layers global variable\n",
    "            \"\"\"\n",
    "            self.t5_encoder = t5_model.encoder\n",
    "            \n",
    "            for i_layer, block in enumerate(self.t5_encoder.block):\n",
    "                if i_layer in freeze_layers:\n",
    "                    for param in block.parameters():\n",
    "                        param.requires_grad = False\n",
    "        else:\n",
    "            self.in_embed = nn.Sequential(nn.Embedding(in_vocab.n, n_hid, padding_idx=in_vocab.pad), drop)\n",
    "            self.pos_embed = nn.Embedding(1 + n_max_in, n_hid) # Use the first position as global vector\n",
    "            self.transformer_layers = nn.ModuleList(TransformerBlock() for _ in range(n_layers))\n",
    "\n",
    "        self.gcn = GCN()\n",
    "\n",
    "        self.decoder = TreeDecoder()\n",
    "\n",
    "        if not use_t5:\n",
    "            self.apply(self.init_weight)\n",
    "\n",
    "    def init_weight(self, m):\n",
    "        if type(m) in [nn.Embedding]:\n",
    "            nn.init.normal_(m.weight, 0, 0.1)\n",
    "\n",
    "    def encode(self, in_idxs, n_in, gt_graph, attr_graph, in_mask=None):\n",
    "        in_idxs_pad = F.pad(in_idxs, (1, 0), value=in_vocab.pad)\n",
    "        if use_t5:\n",
    "            \"\"\"\n",
    "            Call your T5 encoder\n",
    "            \"\"\"\n",
    "#             h, = self.t5_encoder(in_idxs_pad)\n",
    "            h = self.t5_encoder(in_idxs_pad).last_hidden_state\n",
    "        else:\n",
    "            x = self.in_embed(in_idxs_pad) # (n_batch, n_batch_max_in, n_hid)\n",
    "            h = x + self.pos_embed(torch.arange(x.size(1), device=x.device))\n",
    "            for layer in self.transformer_layers:\n",
    "                h = layer(h, mask=in_mask)\n",
    "                \n",
    "        zg, h = h[:, 0], h[:, 1:]\n",
    "        zbar = self.gcn(h, gt_graph, attr_graph)\n",
    "        return zbar, zg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-29T08:31:12.383238Z",
     "start_time": "2020-10-29T08:31:12.342661Z"
    }
   },
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, up):\n",
    "        self.up = up\n",
    "        self.is_root = up is None\n",
    "        self.left = self.right = None\n",
    "        self.ql = self.tl = self.op = None\n",
    "\n",
    "def train(batch, model, opt):\n",
    "    n_batch = len(batch)\n",
    "\n",
    "    n_in = [d['n_in'] for d in batch]\n",
    "    pad = lambda x, value: nn.utils.rnn.pad_sequence(x, batch_first=True, padding_value=value)\n",
    "    in_idxs = pad([d['in_idxs'] for d in batch], in_vocab.pad).to(device)\n",
    "    in_mask = pad([torch.ones(n, dtype=torch.bool) for n in n_in], False).to(device)\n",
    "    nP_in_mask = pad([d['nP_in_mask'] for d in batch], False).to(device)\n",
    "    nP_out_mask = torch.stack([d['nP_out_mask'] for d in batch]).to(device)\n",
    "    \n",
    "    qcomp_graph, qcell_graph = [], []\n",
    "    for d in batch:\n",
    "        \"\"\"\n",
    "        Create qcomp_graph and qcell_graph from d['qcomp_edges'] and d['qcell_edges'] by calling dgl.graph\n",
    "        (see https://docs.dgl.ai/generated/dgl.graph.html)\n",
    "\n",
    "        Note that num_nodes needs to be set to the maximum input length in this batch\n",
    "        \"\"\"\n",
    "        qcomp_graph_i = dgl.graph(d['qcomp_edges'], num_nodes=in_idxs.size(1), device=device)\n",
    "        qcell_graph_i = dgl.graph(d['qcell_edges'], num_nodes=in_idxs.size(1), device=device)\n",
    "        \n",
    "        qcomp_graph.append(qcomp_graph_i)\n",
    "        qcell_graph.append(qcell_graph_i)\n",
    "    qcomp_graph = dgl.batch(qcomp_graph)\n",
    "    qcell_graph = dgl.batch(qcell_graph)\n",
    "    \n",
    "    label = pad([d['out_idxs'] for d in batch], out_vocab.pad)\n",
    "    nP_candidates = [d['nP_candidates'] for d in batch]\n",
    "\n",
    "    \n",
    "    zbar, qroot = model.encode(in_idxs, n_in, qcomp_graph, qcell_graph, in_mask=None)\n",
    "    z_nP = zbar.new_zeros((n_batch, n_max_nP, n_hid))\n",
    "    z_nP[nP_out_mask] = zbar[nP_in_mask]\n",
    "\n",
    "    decoder = model.decoder\n",
    "\n",
    "    n_quant = out_vocab.n_constants + n_max_nP\n",
    "    quant_embed = torch.cat([decoder.constant_embedding.expand(n_batch, -1, -1), z_nP], dim=1) # (n_batch, n_quant, n_hid)\n",
    "\n",
    "    nodes = np.array([Node(None) for _ in range(n_batch)])\n",
    "    op_min, op_max = out_vocab.base_op, out_vocab.base_op + out_vocab.n_ops\n",
    "    quant_min, quant_max = out_vocab.base_quant, out_vocab.base_quant + n_quant\n",
    "\n",
    "    # Initialize root node vector according to zg (the global context)\n",
    "    qp = decoder.qp_gate(qroot)\n",
    "    scores = []\n",
    "    for i, label_i in enumerate(label.T): # Iterate over the output positions\n",
    "        Gc = decoder.gts_attention(qp, zbar, in_mask)\n",
    "        qp_Gc = torch.cat([qp, Gc], dim=1) # (n_batch, 2 * n_hid)\n",
    "\n",
    "        score = decoder.gts_predict(qp_Gc, quant_embed, nP_out_mask)\n",
    "        scores.append(score)\n",
    "\n",
    "        # Whether the label is an operator\n",
    "        is_op = (op_min <= label_i) & (label_i < op_max)\n",
    "        # Whether the label is a quantity\n",
    "        is_quant = ((quant_min <= label_i) & (label_i < quant_max)) | (label_i == out_vocab.unk)\n",
    "\n",
    "        op_embed = decoder.op_embedding((label_i[is_op] - out_vocab.base_op).to(device))\n",
    "        qp_Gc_op = torch.cat([qp_Gc[is_op], op_embed], dim=1)\n",
    "\n",
    "        is_left = np.zeros(n_batch, dtype=np.bool)\n",
    "        qleft_qp = decoder.gts_left_qp(qp_Gc_op)\n",
    "        qleft = decoder.gts_left(qp_Gc_op)\n",
    "        for j, ql, op in zip(is_op.nonzero(as_tuple=True)[0], qleft, op_embed):\n",
    "            node = nodes[j]\n",
    "            nodes[j] = node.left = Node(node)\n",
    "            node.op = op\n",
    "            node.ql = ql\n",
    "            is_left[j] = True\n",
    "\n",
    "        is_right = np.zeros(n_batch, dtype=np.bool)\n",
    "        nP_score = score[:, out_vocab.base_nP:].detach().cpu()\n",
    "        ql_tl = []\n",
    "        for j in is_quant.nonzero(as_tuple=True)[0]:\n",
    "            if label_i[j] == out_vocab.unk:\n",
    "                candidates = nP_candidates[j][i]\n",
    "#                 label_i[j] = out_vocab.base_nP + candidates[nP_score[j, candidates].argmax()]\n",
    "                label_i[j] = torch.from_numpy(np.array(out_vocab.base_nP + candidates[nP_score[j, candidates].argmax()])).to(label_i)\n",
    "\n",
    "            node = nodes[j]\n",
    "            pnode = node.up\n",
    "            t = quant_embed[j, label_i[j] - out_vocab.base_quant]\n",
    "            while pnode and pnode.right is node:\n",
    "                t = decoder.merge_subtree(pnode.op, pnode.tl, t) # merge operator, left subtree, and right child\n",
    "                node, pnode = pnode, pnode.up # backtrack to parent node\n",
    "            if pnode is None: # Finished traversing tree of j\n",
    "                continue\n",
    "            # Now pnode.left is node. t is the tl representing the left subtree of pnode\n",
    "            pnode.tl = t\n",
    "            ql_tl.append(torch.cat([pnode.ql, pnode.tl])) # For computing qright\n",
    "            nodes[j] = pnode.right = Node(pnode)\n",
    "            is_right[j] = True\n",
    "\n",
    "        qp = torch.zeros((n_batch, n_hid), device=device)\n",
    "        qp[is_left] = qleft_qp\n",
    "        if ql_tl:\n",
    "            qp[is_right] = decoder.gts_right(torch.stack(ql_tl))\n",
    "\n",
    "    label = label.to(device).view(-1)\n",
    "    scores = torch.stack(scores, dim=1).view(-1, out_vocab.n_ops + n_quant)\n",
    "    loss = F.cross_entropy(scores, label, ignore_index=out_vocab.pad)\n",
    "\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeamNode(Node):\n",
    "    def __init__(self, up, prev, qp, token=None):\n",
    "        super().__init__(up)\n",
    "        self.prev = prev\n",
    "        self.qp = qp\n",
    "        self.token = token\n",
    "\n",
    "    def trace_tokens(self, *last_token):\n",
    "        if self.prev is None:\n",
    "            return list(last_token)\n",
    "        tokens = self.prev.trace_tokens()\n",
    "        tokens.append(self.token)\n",
    "        tokens.extend(last_token)\n",
    "        return tokens\n",
    "\n",
    "def evaluate(d, model, beam_size=5, n_max_out=45):\n",
    "    in_idxs = d['in_idxs'].unsqueeze(0).to(device=device)\n",
    "    \"\"\"\n",
    "    Create qcomp_graph and qcell_graph from d['qcomp_edges'] and d['qcell_edges'] by calling dgl.graph\n",
    "    (see https://docs.dgl.ai/generated/dgl.graph.html)\n",
    "    \"\"\"\n",
    "#     qcomp_graph = dgl.graph(d['gt_edges'], device=device)\n",
    "#     qcell_graph = dgl.graph(d['attr_edges'], device=device)\n",
    "    qcomp_graph = dgl.graph(d['qcomp_edges'], device=device)\n",
    "    qcell_graph = dgl.graph(d['qcell_edges'], device=device)\n",
    "\n",
    "    zbar, qroot = model.encode(in_idxs, [d['n_in']], qcomp_graph, qcell_graph)\n",
    "    z_nP = zbar[:, d['nP_positions']]\n",
    "\n",
    "    decoder = model.decoder\n",
    "\n",
    "    quant_embed = torch.cat([decoder.constant_embedding, z_nP], dim=1) # (1, n_quant, n_hid)\n",
    "    op_min, op_max = out_vocab.base_op, out_vocab.base_op + out_vocab.n_ops\n",
    "\n",
    "    best_done_beam = (-np.inf, None, None)\n",
    "    beams = [(0, BeamNode(up=None, prev=None, qp=decoder.qp_gate(qroot)))]\n",
    "    for _ in range(n_max_out):\n",
    "        new_beams = []\n",
    "        for logp_prev, node in beams:\n",
    "            Gc = decoder.gts_attention(node.qp, zbar)\n",
    "            qp_Gc = torch.cat([node.qp, Gc], dim=1) # (2 * n_hid,)\n",
    "\n",
    "            log_prob = decoder.gts_predict(qp_Gc, quant_embed).log_softmax(dim=1)\n",
    "            top_logps, top_tokens = log_prob.topk(beam_size, dim=1)\n",
    "            for logp_token_, out_token_ in zip(top_logps.unbind(dim=1), top_tokens.unbind(dim=1)):\n",
    "                out_token = out_token_.item()\n",
    "                logp = logp_prev + logp_token_.item()\n",
    "                if op_min <= out_token < op_max:\n",
    "                    op_embed = decoder.op_embedding(out_token_)\n",
    "                    qp_Gc_op = torch.cat([qp_Gc, op_embed], dim=1)\n",
    "                    prev_node = copy(node)\n",
    "                    next_node = prev_node.left = BeamNode(\n",
    "                        up=prev_node, prev=prev_node,\n",
    "                        qp=decoder.gts_left_qp(qp_Gc_op),\n",
    "                        token=out_token\n",
    "                    )\n",
    "                    prev_node.op = op_embed\n",
    "                    prev_node.ql = decoder.gts_left(qp_Gc_op)\n",
    "                else:\n",
    "                    pnode, prev_node = node.up, node\n",
    "                    t = quant_embed[:, out_token - out_vocab.base_quant]\n",
    "                    while pnode and pnode.tl is not None:\n",
    "                        t = decoder.merge_subtree(pnode.op, pnode.tl, t)\n",
    "                        node, pnode = pnode, pnode.up\n",
    "                    if pnode is None:\n",
    "                        best_done_beam = max(best_done_beam, (logp, prev_node, out_token))\n",
    "                        continue\n",
    "                    pnode = copy(pnode)\n",
    "                    pnode.tl = t\n",
    "                    next_node = pnode.right = BeamNode(\n",
    "                        up=pnode, prev=prev_node,\n",
    "                        qp=decoder.gts_right(torch.cat([pnode.ql, pnode.tl], dim=1)),\n",
    "                        token=out_token\n",
    "                    )\n",
    "                new_beams.append((logp, next_node))\n",
    "        beams = sorted(new_beams, key=lambda x: x[0], reverse=True)[:beam_size]\n",
    "        done_logp, done_node, done_last_token = best_done_beam\n",
    "        if not len(beams) or done_logp >= beams[0][0]:\n",
    "            break\n",
    "    return done_node.trace_tokens(done_last_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(\"data/question-to-topic-cleaned.json\", \"r\") as f:\n",
    "    question_to_topic = eval(f.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_chars = {',', '.', ' ', 'negative', '-', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9'}\n",
    "def cleaner(q):\n",
    "    for char in delete_chars:\n",
    "        q = q.replace(char, '')\n",
    "    return q\n",
    "    \n",
    "def score_model(model, test_data):\n",
    "    model.eval()\n",
    "    value_match, equation_match = [], []\n",
    "    with torch.no_grad():\n",
    "        for d in tqdm(test_data):\n",
    "            val_match = eq_match = False\n",
    "            if not d['is_quadratic']: # This method is not equiped to handle equations with quadratics\n",
    "                try:\n",
    "                    pred = evaluate(d, model)\n",
    "                    d['pred_tokens'] = [out_vocab.idx2token[idx] for idx in pred]\n",
    "                    val_match, eq_match = check_match(pred, d)\n",
    "                except:\n",
    "                    pass\n",
    "            value_match.append(val_match)\n",
    "            equation_match.append(eq_match)\n",
    "    print(f'Test equation accuracy: {np.mean(equation_match):.3g} (not weighted equally across topics)')\n",
    "    print(f'Test value accuracy: {np.mean(value_match):.3g} (not weighted equally across topics)')\n",
    "    \n",
    "def score_model_detailed(model, test_data):\n",
    "    model.eval()\n",
    "    value_match, equation_match = [], []\n",
    "    correct_val_topics = []\n",
    "    correct_eqn_topics = []\n",
    "    correct_questions_val = {} # topic -> (question, answer)\n",
    "    incorrect_questions_val = {} # topic -> (question, answer)\n",
    "    topic_count_in_test = {}\n",
    "    with torch.no_grad():\n",
    "        for d in tqdm(test_data):\n",
    "            val_match = eq_match = False\n",
    "            parse_tree = None\n",
    "            if not d['is_quadratic']: # This method is not equiped to handle equations with quadratics\n",
    "                try:\n",
    "                    pred = evaluate(d, model)\n",
    "                    d['pred_tokens'] = [out_vocab.idx2token[idx] for idx in pred]\n",
    "                    val_match, eq_match = check_match(pred, d)\n",
    "                    parse_tree = sub_nP(d['pred_tokens'], d['nP'])\n",
    "                except:\n",
    "                    pass\n",
    "            value_match.append(val_match)\n",
    "            equation_match.append(eq_match)\n",
    "            \n",
    "            cleaned_question_topic = question_to_topic[cleaner(d['processed_question'])]\n",
    "            topic_count_in_test[cleaned_question_topic] = topic_count_in_test.get(cleaned_question_topic, 0) + 1\n",
    "            if val_match:\n",
    "                correct_val_topics.append(cleaned_question_topic)\n",
    "                correct_questions_val[cleaned_question_topic] = correct_questions_val.get(cleaned_question_topic, []) + [(d['processed_question'], evaluate_prefix_expression(parse_tree))]\n",
    "            else:\n",
    "                try:\n",
    "                    incorrect_questions_val[cleaned_question_topic] = incorrect_questions_val.get(cleaned_question_topic, []) + [(d['processed_question'], evaluate_prefix_expression(parse_tree))]\n",
    "                except:\n",
    "                    incorrect_questions_val[cleaned_question_topic] = incorrect_questions_val.get(cleaned_question_topic, []) + [(d['processed_question'], parse_tree)]\n",
    "            if eq_match:\n",
    "                correct_eqn_topics.append(cleaned_question_topic)\n",
    "                \n",
    "    correct_eqn_topics = Counter(correct_eqn_topics)\n",
    "    correct_eqn_percents = {topic: questions_correct/topic_count_in_test[topic] for topic,questions_correct in correct_eqn_topics.items()}\n",
    "    print(f'Test equation accuracy: {(sum([correct_eqn_percents[topic] for topic in correct_eqn_topics])/len(topics)):.3g}')\n",
    "    print(f'Test equation accuracy per topic: {correct_eqn_percents}')\n",
    "          \n",
    "    correct_val_topics = Counter(correct_val_topics)\n",
    "    correct_val_percents = {topic: questions_correct/topic_count_in_test[topic] for topic,questions_correct in correct_val_topics.items()}\n",
    "    print(f'Test value accuracy: {(sum([correct_val_percents[topic] for topic in correct_val_topics])/len(topics)):.3g}')\n",
    "    print(f'Test value accuracy per topic: {correct_val_percents}')\n",
    "    \n",
    "    return correct_questions_val, incorrect_questions_val\n",
    "    \n",
    "    \n",
    "def score_model_single_input_fr(model, question):\n",
    "    model.eval()\n",
    "    if question[-1] not in {'.', '?'}:\n",
    "        question += \"?\"\n",
    "    d = [{\"expression\": \"\", \"quant_cell_positions\": [i for i in range(len(question.split(\" \")))], \"processed_question\": question, \"raw_question\": question}]\n",
    "    _, input_question_data, _, _, _, _ = setup(use_t5, test_split=1, data=d)\n",
    "    tensorize_data([], input_question_data)\n",
    "    input_question = input_question_data[0]\n",
    "    pred = evaluate(input_question, model)\n",
    "    d[0]['pred_tokens'] = [out_vocab.idx2token[idx] for idx in pred]\n",
    "    parse_tree = sub_nP(d[0]['pred_tokens'], d[0]['nP'])\n",
    "    return str(evaluate_prefix_expression(parse_tree)), parse_tree\n",
    "\n",
    "def score_model_single_input_mc(model, question, solution_tree, answers_generated=20, num_choices=4, verbose=True):\n",
    "    model.eval()\n",
    "    if question[-1] not in {'.', '?'}:\n",
    "        question += \"?\"\n",
    "    d = [{\"expression\": \"\", \"quant_cell_positions\": [i for i in range(len(question.split(\" \")))], \"processed_question\": question, \"raw_question\": question}]\n",
    "    _, input_question_data, _, _, _, _ = setup(use_t5, test_split=1, data=d)\n",
    "    tensorize_data([], input_question_data)\n",
    "    input_question = input_question_data[0]\n",
    "    \n",
    "    result = []\n",
    "    for _ in range(answers_generated):\n",
    "        try:\n",
    "            pred = evaluate(d[0], model)\n",
    "            res = sub_nP([out_vocab.idx2token[idx] for idx in pred], d[0]['nP'])\n",
    "            res = evaluate_prefix_expression(res)\n",
    "            result.append(str(res))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "    counts = Counter(result)\n",
    "    counts = sorted(counts.items(), key=lambda x: eval(x[0]))\n",
    "    preds = [eval(elt[0]) for elt in sorted(counts, key=lambda x: (x[1], random.random()), reverse=True)][:min(len(counts),num_choices)]\n",
    "    \n",
    "    correct_tree = sub_nP(solution_tree, d[0]['nP'])\n",
    "    # generate answer choices\n",
    "    answers, correct_answer = generate_choices(correct_tree, num_choices)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Correct answer: {correct_answer}\")\n",
    "        print(f\"Answers: {answers}\")\n",
    "    \n",
    "    # have model make decision\n",
    "    for try_number in range(num_choices):\n",
    "        chosen_answer = random.choice(answers)\n",
    "        for pred in preds:\n",
    "            if pred in answers:\n",
    "                chosen_answer = pred\n",
    "                break # if not chosen after iterating over all preds, keep the chosen one at random\n",
    "        del answers[answers.index(chosen_answer)]\n",
    "        if verbose:\n",
    "            print(f\"Try #{try_number+1}: {chosen_answer}\")\n",
    "        if chosen_answer == correct_answer:\n",
    "            return (num_choices - try_number - 1)/(num_choices-1), try_number + 1\n",
    "    \n",
    "def score_model_ranking_multiple_choice(model, test_data, num_tries=4, answers_generated=10, num_choices=4):\n",
    "    model.eval()\n",
    "    value_match = []\n",
    "    tries = []\n",
    "    tries_per_topic = {}\n",
    "    topic_to_score = {}\n",
    "    topic_count_in_test = {}\n",
    "    with torch.no_grad():\n",
    "        for d in tqdm(test_data):\n",
    "            if d['is_quadratic']: # This method is not equipped to handle equations with quadratics\n",
    "                val_match = eq_match = False\n",
    "                try_number = num_tries - 1\n",
    "            else:\n",
    "                # generate responses\n",
    "                result = []\n",
    "                for _ in range(answers_generated):\n",
    "                    try:\n",
    "                        pred = evaluate(d, model)\n",
    "                        res = sub_nP([out_vocab.idx2token[idx] for idx in pred], d['nP'])\n",
    "                        res = evaluate_prefix_expression(res)\n",
    "                        result.append(str(res))\n",
    "                    except:\n",
    "                        pass\n",
    "                counts = Counter(result)\n",
    "                counts = sorted(counts.items(), key=lambda x: eval(x[0]))\n",
    "                preds = [eval(elt[0]) for elt in sorted(counts, key=lambda x: (x[1], random.random()), reverse=True)][:min(len(counts),num_tries)]\n",
    "                # generate answer choices\n",
    "                val_match = 0\n",
    "                question_topic = question_to_topic[cleaner(d['raw_question'])]\n",
    "                topic_count_in_test[question_topic] = topic_count_in_test.get(question_topic, 0) + 1\n",
    "                try:\n",
    "                    correct_tree = sub_nP(d['out_tokens'], d['nP'])\n",
    "                    answers, correct_answer = generate_choices(correct_tree, num_choices)\n",
    "                    # have model make decision\n",
    "                    for try_number in range(len(preds)):\n",
    "                        chosen_answer = random.choice(answers)\n",
    "                        for pred in preds:\n",
    "                            if pred in answers:\n",
    "                                chosen_answer = pred\n",
    "                                break # if not chosen after iterating over all preds, keep the chosen one at random\n",
    "                        del answers[answers.index(chosen_answer)]\n",
    "                        if chosen_answer == correct_answer:\n",
    "                            val_match = (num_choices - try_number - 1)/(num_choices-1)\n",
    "                            tries.append(try_number + 1)\n",
    "                            tries_per_topic[question_topic] = tries_per_topic.get(question_topic, []) + [try_number + 1]\n",
    "                            topic_to_score[question_topic] = topic_to_score.get(question_topic, 0) + val_match\n",
    "                            break\n",
    "                except:\n",
    "                    pass\n",
    "    correct_val_percents = {topic: topic_to_score[topic]/topic_count_in_test[topic] for topic in topic_to_score}\n",
    "    sum_scores = sum([topic_to_score[topic] for topic in topic_to_score])\n",
    "    sum_test_data_count = sum([topic_count_in_test[topic] for topic in topic_count_in_test])\n",
    "    print(f'Test value accuracy: {(sum_scores/sum_test_data_count):.3g} (over the test set)')\n",
    "    print(f'Test value accuracy: {(sum([correct_val_percents[topic] for topic in correct_val_percents])/len(topics)):.3g} (weighted equally across topics)')\n",
    "    print(f'Test value accuracy per topic: {correct_val_percents}')\n",
    "    \n",
    "def generate_choices(parse_tree, num_choices, operators=None):\n",
    "    if operators is None:\n",
    "        operators = {'+': np.add, '-': np.subtract, '*': np.multiply, '/': np.divide, 'm': max, 'l':math.log, '^': np.power}\n",
    "    special_values = [0.1, 0.2, 0.25, 0.4, 0.5, 0.6, 0.75, 0.8, 2, 2.5, 3, 4, 5, 6, 7.5, 8, 10]\n",
    "    correct_answer = evaluate_prefix_expression(parse_tree)\n",
    "    parse_tree = [elt if elt in operators else eval(elt) for elt in parse_tree]\n",
    "    \n",
    "    # find all other possible parse tree constructions\n",
    "    good_trees = [parse_tree]\n",
    "    bad_trees = []\n",
    "    valid_answers = []\n",
    "    for _ in range(1):\n",
    "        good_trees_size = len(good_trees)\n",
    "        for tree in good_trees:\n",
    "            for idx in range(len(tree)-4):\n",
    "                if parse_tree[idx] in operators:\n",
    "                    if parse_tree[idx+1] in operators: # zig rotation\n",
    "                        proposed_tree = zig_rotation(parse_tree, idx)\n",
    "                    if parse_tree[idx+2] in operators: # zag rotation\n",
    "                        proposed_tree = zag_rotation(parse_tree, idx)\n",
    "                    if proposed_tree not in good_trees and proposed_tree not in bad_trees:\n",
    "                        try:\n",
    "                            answer = evaluate_prefix_expression(answer_tree)\n",
    "                            good_trees.append(proposed_tree)\n",
    "                            valid_answers.append(answer)\n",
    "                        except:\n",
    "                            bad_trees.append(proposed_tree)\n",
    "                        if len(good_trees) >= num_choices:\n",
    "                            break\n",
    "            if len(good_trees) >= num_choices:\n",
    "                break\n",
    "        if good_trees_size == len(good_trees) or len(good_trees) >= num_choices:\n",
    "            break\n",
    "    num_attempts = 0\n",
    "    while len(valid_answers) < num_choices:\n",
    "        if num_attempts > 2*num_choices:\n",
    "            valid_answers += random.sample(special_values, num_choices-len(valid_answers))\n",
    "            break\n",
    "        num_attempts += 1\n",
    "        numeric_idxs = [idx for idx in range(len(parse_tree)) if parse_tree[idx] not in operators]\n",
    "        numeric_idx = random.choice(numeric_idxs)\n",
    "        parse_tree_augmented = parse_tree.copy()\n",
    "        option = random.random()\n",
    "        if option < 2/3:\n",
    "            parse_tree_augmented[numeric_idx] *= random.choice(special_values)\n",
    "        elif 1/3 < option:\n",
    "            parse_tree_augmented[numeric_idx] += random.choice(special_values)\n",
    "        else:\n",
    "            parse_tree_augmented[numeric_idx] -= random.choice(special_values)\n",
    "        try:\n",
    "            result = evaluate_prefix_expression(parse_tree_augmented)\n",
    "            if result in valid_answers or result == correct_answer:\n",
    "                continue\n",
    "            else:\n",
    "                valid_answers.append(result)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        \n",
    "    random.shuffle(valid_answers)\n",
    "    correct_idx = int(random.random()*num_choices)\n",
    "    valid_answers[correct_idx] = correct_answer\n",
    "    return valid_answers[:num_choices], correct_answer\n",
    "    \n",
    "def zig_rotation(parse_tree, first_rotation_idx):\n",
    "    tree = parse_tree.copy()\n",
    "    tree[first_rotation_idx], tree[first_rotation_idx+1], tree[first_rotation_idx+2] = tree[first_rotation_idx+1], tree[first_rotation_idx+2], tree[first_rotation_idx]\n",
    "    return tree\n",
    "\n",
    "def zag_rotation(parse_tree, first_rotation_idx):\n",
    "    tree = parse_tree.copy()\n",
    "    tree[first_rotation_idx], tree[first_rotation_idx+1], tree[first_rotation_idx+2] = tree[first_rotation_idx+2], tree[first_rotation_idx], tree[first_rotation_idx+1]\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "use_t5 = None\n",
    "\n",
    "n_max_in = 100\n",
    "n_batch = 32\n",
    "learning_rate = 1e-4\n",
    "if use_t5:\n",
    "    # T5 hyperparameters\n",
    "    n_epochs = 50\n",
    "    freeze_layers = []\n",
    "    weight_decay = 1e-5\n",
    "    n_hid = dict(small=512, base=768)[use_t5] # Do not modify unless you want to try t5-large\n",
    "else:\n",
    "    # Custom transformer hyperparameters\n",
    "    n_epochs = 25\n",
    "    n_layers = 3\n",
    "    n_hid = 512\n",
    "    n_k = n_v = 64\n",
    "    n_head = 8\n",
    "    weight_decay = 0\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "topics = [\"b\", \"p\", \"f\", \"lr\", \"r\", \"nn_i\", \"nn_ii\", \"cnn\", \"sm_mdp\", \"rl\", \"rnn\", \"dtnn\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create from json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tensorizing...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ebb2e58f7ea42339dfa17b6a09c4848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data, in_vocab, out_vocab, n_max_nP, t5_model = setup(use_t5, path=\"data/train-cleaned.json\")\n",
    "print(\"\\nTensorizing...\")\n",
    "tensorize_data(train_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to setup_data_split_new.pickle\n",
      "Saved to setup_vocab_model_t5_none_new.pickle\n"
     ]
    }
   ],
   "source": [
    "def save_data_split():\n",
    "    filename = 'setup_data_split_new.pickle'\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump([train_data, test_data, n_max_nP], f)\n",
    "        print(f'Saved to {filename}')\n",
    "        \n",
    "def save_vocab_model():\n",
    "    suffix = str(use_t5) if use_t5 is not None else \"none\"\n",
    "    filename = f'setup_vocab_model_t5_{suffix}_new.pickle'\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump([in_vocab, out_vocab, t5_model], f)\n",
    "        print(f'Saved to {filename}')\n",
    "if not use_t5:\n",
    "    save_data_split() # if using T5, no need to save data split again\n",
    "save_vocab_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening setup_data_split_new.pickle\n",
      "Opening setup_vocab_model_t5_none_new.pickle\n"
     ]
    }
   ],
   "source": [
    "def load_data_split(split_file):\n",
    "    with open(split_file, 'rb') as f:\n",
    "        print(f\"Opening {split_file}\")\n",
    "        return pickle.load(f)\n",
    "        \n",
    "def load_vocab_model(vocab_file):\n",
    "    with open(vocab_file, 'rb') as f:\n",
    "        print(f\"Opening {vocab_file}\")\n",
    "        return pickle.load(f)\n",
    "train_data, test_data, n_max_nP = load_data_split('setup_data_split_new.pickle')\n",
    "in_vocab, out_vocab, t5_model = load_vocab_model('setup_vocab_model_t5_none_new.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df9d07cd1e284b639f3ed4e12697beb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=30.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f720fa009c7e4651b4e23b72fde7a740",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=350.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training loss: 0.057144755236804484\n",
      "Epoch: 17\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0bedecbbd394360b3c8ff604394e377",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=350.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training loss: 0.051736201447035585\n",
      "Epoch: 18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0ba26f336f940ef9841e487a215931f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=350.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training loss: 0.047699078058025667\n",
      "Epoch: 19\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "818884f403f84827a8432696e605ca5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=350.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training loss: 0.04332305947318673\n",
      "Epoch: 20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0c775a6703e4aaa96fcca83c40252c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=350.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training loss: 0.04165648732068283\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fb02784944d49c483dcc38c38ec0ec4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2800.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test equation accuracy: 0.898\n",
      "Test equation accuracy per topic: {'p': 0.9951690821256038, 'dtnn': 1.0, 'rnn': 1.0, 'cnn': 0.8423645320197044, 'lr': 0.8137254901960784, 'f': 0.7568493150684932, 'r': 0.954225352112676, 'nn_i': 0.9858490566037735, 'nn_ii': 0.9574468085106383, 'b': 1.0, 'sm_mdp': 0.47115384615384615, 'rl': 1.0}\n",
      "Test value accuracy: 0.946\n",
      "Test value accuracy per topic: {'p': 0.9951690821256038, 'dtnn': 1.0, 'rnn': 1.0, 'cnn': 0.8637110016420362, 'lr': 0.826797385620915, 'f': 0.7636986301369864, 'r': 0.954225352112676, 'nn_i': 0.9858490566037735, 'nn_ii': 0.9574468085106383, 'b': 1.0, 'sm_mdp': 1.0, 'rl': 1.0}\n",
      "Epoch: 21\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80bdcae45cc748e79bbf738c2e94833b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=350.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training loss: 0.03850527690989631\n",
      "Epoch: 22\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5633ba76d66a44f0842a38581b36be39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=350.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training loss: 0.03544869997006442\n",
      "Epoch: 23\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "106bcfeee23a48079390805b7a376019",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=350.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training loss: 0.03453610876708158\n",
      "Epoch: 24\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06e7dca6a96b4a3ebde0492df17e86d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=350.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training loss: 0.03298065975308418\n",
      "Epoch: 25\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b10839e705b24cd8adbe3a3866fb3426",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=350.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training loss: 0.03125473217772586\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a6c03a338fd44819dd689b4936d5880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2800.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test equation accuracy: 0.951\n",
      "Test equation accuracy per topic: {'p': 0.9806763285024155, 'dtnn': 1.0, 'rnn': 1.0, 'cnn': 0.8390804597701149, 'lr': 0.8594771241830066, 'f': 0.8561643835616438, 'r': 0.971830985915493, 'nn_i': 1.0, 'nn_ii': 0.9680851063829787, 'b': 1.0, 'sm_mdp': 0.9423076923076923, 'rl': 1.0}\n",
      "Test value accuracy: 0.958\n",
      "Test value accuracy per topic: {'p': 0.9806763285024155, 'dtnn': 1.0, 'rnn': 1.0, 'cnn': 0.8571428571428571, 'lr': 0.8627450980392157, 'f': 0.8561643835616438, 'r': 0.971830985915493, 'nn_i': 1.0, 'nn_ii': 0.9680851063829787, 'b': 1.0, 'sm_mdp': 1.0, 'rl': 1.0}\n",
      "Epoch: 26\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e922a6e3091741f686ee4ddb6b111190",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=350.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-76fd73160e15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'n_in'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-6b7597cb49f0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(batch, model, opt)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "DIR_TO_SAVE_TO = 'new'\n",
    "os.makedirs(f'models/{DIR_TO_SAVE_TO}', exist_ok=True)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "model = Model()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(opt, n_epochs)\n",
    "model.to(device)\n",
    "\n",
    "starting_epoch = 0\n",
    "for epoch in tqdm(range(starting_epoch, starting_epoch + n_epochs)):\n",
    "    print('Epoch:', epoch + 1)\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for start in trange(0, int(len(train_data)), n_batch):\n",
    "        batch = sorted(train_data[start: start + n_batch], key=lambda d: -d['n_in'])\n",
    "        loss = train(batch, model, opt)\n",
    "        losses.append(loss)\n",
    "    scheduler.step()\n",
    "\n",
    "    print('Training loss:', np.mean(losses))\n",
    "\n",
    "    epoch += 1\n",
    "    torch.save(model.state_dict(), f'models/{DIR_TO_SAVE_TO}/model-{epoch}.pth')\n",
    "    if epoch % 5 == 0:\n",
    "#         torch.save(model.state_dict(), f'models/model-{epoch}-t5_{str(use_t5) if use_t5 is not None else \"none\"}_cleanedx3.pth')\n",
    "        score_model_detailed(model, test_data)\n",
    "#         score_model_ranking_multiple_choice(model, test_data[:int(len(test_data)/1000)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading a Saved Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model()\n",
    "\n",
    "# model.load_state_dict(torch.load(\"models/final/model-15-t5_none_final.pth\"))\n",
    "model.load_state_dict(torch.load(\"models/new/model-25.pth\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "025fc758d4e944e9a2e6c84f6138dff3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2800.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test equation accuracy: 0.925 (not weighted equally across topics)\n",
      "Test value accuracy: 0.931 (not weighted equally across topics)\n"
     ]
    }
   ],
   "source": [
    "score_model(model, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "889adf048a954a95bec7bee276c19757",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2800.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test equation accuracy: 0.951\n",
      "Test equation accuracy per topic: {'p': 0.9806763285024155, 'dtnn': 1.0, 'rnn': 1.0, 'cnn': 0.8390804597701149, 'lr': 0.8594771241830066, 'f': 0.8561643835616438, 'r': 0.971830985915493, 'nn_i': 1.0, 'nn_ii': 0.9680851063829787, 'b': 1.0, 'sm_mdp': 0.9423076923076923, 'rl': 1.0}\n",
      "Test value accuracy: 0.958\n",
      "Test value accuracy per topic: {'p': 0.9806763285024155, 'dtnn': 1.0, 'rnn': 1.0, 'cnn': 0.8571428571428571, 'lr': 0.8627450980392157, 'f': 0.8561643835616438, 'r': 0.971830985915493, 'nn_i': 1.0, 'nn_ii': 0.9680851063829787, 'b': 1.0, 'sm_mdp': 1.0, 'rl': 1.0}\n"
     ]
    }
   ],
   "source": [
    "correct_questions_val, incorrect_questions_val = score_model_detailed(model, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d162b0b7dac4f2c8823f132236c1d65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2800.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sunnyt/solving-mlp/util.py:211: RuntimeWarning: divide by zero encountered in power\n",
      "  return fn(arg1, arg2), end\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test value accuracy: 0.947 (over the test set)\n",
      "Test value accuracy: 0.968 (weighted equally across topics)\n",
      "Test value accuracy per topic: {'p': 0.9830917874396136, 'dtnn': 1.0, 'rnn': 1.0, 'cnn': 0.8932676518883416, 'lr': 0.8954248366013072, 'f': 0.8904109589041096, 'r': 0.971830985915493, 'nn_i': 1.0, 'nn_ii': 0.9787234042553191, 'b': 1.0, 'sm_mdp': 1.0, 'rl': 1.0}\n"
     ]
    }
   ],
   "source": [
    "score_model_ranking_multiple_choice(model, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Own Input: Free Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e4a42473bb14a1c8cc75fb4e5a97b6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('-2.0', ['+', '*', '2', '1', '*', '4', '-', '0', '1'])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_model_single_input_fr(model, \"A point p is classified by a classifier whose decision boundary is theta = ( 2 4 ) . How does it classify p , where p is ( 1 negative 1 ) ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 195.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'expression': '', 'quant_cell_positions': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29], 'processed_question': 'A classifier has a decision boundary where theta is ( 0 1 ) . What value does it classify p , where p is ( 2 negative 4 ) ?', 'raw_question': 'A classifier has a decision boundary where theta is ( 0 1 ) . What value does it classify p , where p is ( 2 negative 4 ) ?'}\n",
      "Number of items: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('-4.0', ['-', '+', '*', '0', '2', '*', '1', '0', '4'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_model_single_input_fr(model, \"A classifier has a decision boundary where theta is ( 0 1 ) . What value does it classify p , where p is ( 2 negative 4 ) ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Own Input: Multiple Choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 236.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.280109889280518]\n",
      "Correct answer: 1.4142135623730951\n",
      "Answers: [10.04987562112089, 1.004987562112089, 1.4142135623730951, 5.0990195135927845]\n",
      "Try #1: 1.004987562112089\n",
      "Try #2: 1.4142135623730951\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6666666666666666, 2)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What is the magnitude of the vector [ 1 6 4 ] ?\"\n",
    "solution = ['^', '+', '^', '1', '2', '^', '1', '2', '0.5']\n",
    "solutions_generated = 100\n",
    "num_tries = 4\n",
    "score_model_single_input_mc(model, question, solution, solutions_generated, num_tries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 158.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20.0]\n",
      "Correct answer: 20.0\n",
      "Answers: [16.8, 32.5, 20.0, 60.0]\n",
      "Try #1: 20.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.0, 1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Do the two classifiers [ 0 1 0 ] and [ 2 2 0 ] represent the same hyperplane ? Return 1 if true and anything else otherwise .\"\n",
    "solution = ['*', '*', '-', '1', '*', '-', '0', '2', '+', '0', '2', '-', '1', '*', '-', '1', '2', '+', '1', '2', '-', '1', '*', '-', '0', '0', '+', '0', '0']\n",
    "solutions_generated = 100\n",
    "num_tries = 4\n",
    "score_model_single_input_mc(model, question, solution, solutions_generated, num_tries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 212.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23.0]\n",
      "Correct answer: 23.0\n",
      "Answers: [23.0, 25.5, 192.0, 9.2]\n",
      "Try #1: 23.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.0, 1)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Using a stride length of 2 , what is the output from applying a filter of length 7 to an image of length 52 ?\"\n",
    "solution = ['/', '+', '-', '52', '7', '1', '2']\n",
    "solutions_generated = 100\n",
    "num_tries = 4\n",
    "score_model_single_input_mc(model, question, solution, solutions_generated, num_tries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 70.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct answer: 0.0\n",
      "Answers: [9.6, 0.0, 12.8, 40.0]\n",
      "Try #1: 40.0\n",
      "Try #2: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6666666666666666, 2)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"A state machine is defined by the equations s_t = f(s_(t-1), x_t) and y_t = g(s_t) . Given the conditions s_0 = 16 , f(s_(t-1), x_t) = max ( s_(t-1) , x_t ) , and g(s_t) = 0 * s_t , compute y_3 if the input is x_t = [ 8 7 9 ] .\"\n",
    "solution = ['*', '0', 'm', 'm', 'm', '16', '8', '7', '9']\n",
    "solutions_generated = 100\n",
    "num_tries = 4\n",
    "score_model_single_input_mc(model, question, solution, solutions_generated, num_tries)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
