{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SolvingCodingFINAL.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dSoDbmSlHUh"
      },
      "source": [
        "The following notebook is a demonstration as to how the model solves coding questions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRTHwNIik18C"
      },
      "source": [
        "# Importing relevant packages\n",
        "\n",
        "from copy import copy\n",
        "import os\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "!pip install dgl\n",
        "!pip install dgl-cu100\n",
        "\n",
        "import dgl\n",
        "from dgl.nn import GraphConv\n",
        "\n",
        "!pip install transformers\n",
        "!pip install spacy\n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load('en')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksba_Hi6lb0c"
      },
      "source": [
        "The data to be worked with has the following attributes:\n",
        "1.   raw_question: The question as it was written.\n",
        "2.   inputs: The input arguments of the function to complete.\n",
        "3.   AST: An Abstract Syntax Tree that represents a possible correct program.\n",
        "\n",
        "The following functions preprocess the data for easier future use. We use spaCy for dependency parsing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXv7_1pjmT2y"
      },
      "source": [
        "def tokenize_and_separate_quants(data, n_min_vocab):\n",
        "    pattern = re.compile('\\d+,\\d+|\\d*\\.\\d+|\\d+')\n",
        "    constant_counts = Counter()\n",
        "    n_max_inputs = 0 # Maximum number of quantities in the input\n",
        "    for d in data:\n",
        "        question = d['processed_question'].strip()\n",
        "        d['in_tokens'] = in_tokens = []\n",
        "        inputs = d['inputs']\n",
        "        \n",
        "        # Change this into inputs\n",
        "        \"\"\"\n",
        "        inputs = []\n",
        "        for token in question.split(' '):\n",
        "            if pattern.fullmatch(token):\n",
        "                nP.append(float(token.replace(',', '')))\n",
        "                in_tokens.append('NUM')\n",
        "            else:\n",
        "                in_tokens.append(token)\n",
        "        inputs = np.array(inputs)\n",
        "        \"\"\"\n",
        "\n",
        "        n_max_inputs = max(len(inputs), n_max_inputs)\n",
        "\n",
        "        if 'expression' in d:\n",
        "            expression = d['expression'] # Ground truth expression\n",
        "            # Find all quantities in the expression\n",
        "            out_ops = pattern.split(expression)\n",
        "            out_quants = map(float, pattern.findall(expression))\n",
        "            out_tokens = []\n",
        "            for op, out_quant in itertools.zip_longest(out_ops, out_quants, fillvalue=None):\n",
        "                out_tokens.extend(op)\n",
        "                if out_quant is not None: # The last out_quant is None due to zip_longest\n",
        "                    equals, = np.nonzero(out_quant == nP)\n",
        "                    if len(equals) == 0: # Output quantity not found in the input. Record quantity as constant\n",
        "                        constant_counts[out_quant] += 1\n",
        "                        out_tokens.append(f'{out_quant:g}')\n",
        "                    else:\n",
        "                        out_tokens.append(tuple(equals))\n",
        "            d['out_tokens'] = out_tokens\n",
        "\n",
        "        d['nP'] = np.array([f'{x:g}' for x in nP])\n",
        "        # d['nP_positions'], = (np.array(in_tokens) == 'NUM').nonzero()\n",
        "    constants = ['%g' % n for n, count in constant_counts.items() if count >= n_min_vocab]\n",
        "    return constants, n_max_inputs\n",
        "\n",
        "def dependency_parse(data):\n",
        "    \"\"\"\n",
        "    Given raw text, spaCy takes care of dependency parsing, which will be used \n",
        "    to construct the input graph.\n",
        "    \"\"\"\n",
        "    doc = nlp(data['raw_question'])\n",
        "    d['dependency_parse'] = [token.head.i for token in doc]\n",
        "    d['d_tokens'] = [token.text for token in doc] # Not related to dep parse\n",
        "    return d['dependency_parse']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEOJYCdfnEKT"
      },
      "source": [
        "# Vocabulary\n",
        "\n",
        "The following classes deal with the Vocabulary being used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEcvESJTnKUo"
      },
      "source": [
        "class Vocabulary:\n",
        "    def __init__(self, words, pad='<pad>', unk='<unk>'):\n",
        "        self.idx2token = words\n",
        "        self.token2idx = {w: i for i, w in enumerate(words)}\n",
        "        self.pad = self.token2idx.get(pad, None)\n",
        "        self.unk = self.token2idx.get(unk, None)\n",
        "        self.n = len(words)\n",
        "\n",
        "def convert_word_to_bytepair_tokenization(d, t5_tokenizer):\n",
        "    \"\"\"\n",
        "    Bytepair Tokenization is a common scheme used to reduce vocabulary size in \n",
        "    large pretrained models such as BERT; this is needed for T5 encoding.\n",
        "    \"\"\"\n",
        "    import difflib\n",
        "    t5_space = '‚ñÅ'\n",
        "    d_tokens = d['d_tokens']\n",
        "    question = d['raw_question']\n",
        "\n",
        "    t_tokens = [x.replace(t5_space, '') for x in t5_tokenizer.tokenize(question)]\n",
        "    t_tokens = [x for x in t_tokens if x]\n",
        "\n",
        "    t_join = ''.join(t_tokens)\n",
        "    d_join = ''.join(d_tokens)\n",
        "    if t_join == d_join:\n",
        "        t2d = np.arange(len(t_join)).reshape(-1, 1)\n",
        "        d2t = np.arange(len(d_join)).reshape(-1, 1)\n",
        "    else:\n",
        "        i_t = i_d = 0\n",
        "        t2d = np.empty((len(t_join),), dtype=object)\n",
        "        d2t = np.empty((len(d_join),), dtype=object)\n",
        "\n",
        "        to_add = []\n",
        "        to_sub = []\n",
        "        for diff, _, char in difflib.ndiff(t_join, d_join):\n",
        "            if diff == '+':\n",
        "                to_add.append(i_d)\n",
        "                i_d += 1\n",
        "            elif diff == '-':\n",
        "                to_sub.append(i_t)\n",
        "                i_t += 1\n",
        "            else:\n",
        "                for i_d_ in to_add:\n",
        "                    d2t[i_d_] = to_sub\n",
        "                for i_t_ in to_sub:\n",
        "                    t2d[i_t_] = to_add\n",
        "                to_add = []\n",
        "                to_sub = []\n",
        "\n",
        "                t2d[i_t] = [i_d]\n",
        "                d2t[i_d] = [i_t]\n",
        "                i_t += 1\n",
        "                i_d += 1\n",
        "        for i_d_ in to_add:\n",
        "            d2t[i_d_] = to_sub\n",
        "        for i_t_ in to_sub:\n",
        "            t2d[i_t_] = to_add\n",
        "        assert i_t == len(t_join) and i_d == len(d_join)\n",
        "\n",
        "    t_pos = np.concatenate([np.full((len(token),), i) for i, token in enumerate(t_tokens)])\n",
        "\n",
        "    d2t_splits = np.split(d2t, np.cumsum([len(dtok) for dtok in d_tokens])[:-1])\n",
        "    d_pos_to_t_pos = []\n",
        "    for i_d, split in enumerate(d2t_splits):\n",
        "        id_t_pos = set(t_pos[i_t] for i_ts in split for i_t in i_ts)\n",
        "        d_pos_to_t_pos.append(sorted(id_t_pos))\n",
        "\n",
        "    # Convert indices\n",
        "    d['quant_cell_positions'] = [x for qc_pos in d['quant_cell_positions'] for x in d_pos_to_t_pos[qc_pos]]\n",
        "    d['nP_positions'] = [d_pos_to_t_pos[nP_pos][0] for nP_pos in d['nP_positions']]\n",
        "    \n",
        "    # Edits the dependency parse to deal with byte-pair tokenization\n",
        "    new_parse = {}\n",
        "    for d_pos in d_pos_to_t_pos:\n",
        "        for idx, t_pos in enumerate(d_pos_to_t_pos[d_pos]):\n",
        "            if idx:\n",
        "                new_parse[t_pos] = d_pos_to_t_pos[d_pos][idx - 1]\n",
        "            else:\n",
        "                new_parse[t_pos] = d['dependency_parse'][d_pos]\n",
        "    \n",
        "    d['dependency_parse'] = [new_parse[idx] for idx in sorted(new_parse.keys())]\n",
        "    d['in_tokens'] = t_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5L-GaIGmnaJq"
      },
      "source": [
        "# Data Tensorization\n",
        "\n",
        "The following code now takes the preprocessed data and creates tensors that can be inputted into PyTorch models and dgl Graph-related layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQbGQmj_ndFG"
      },
      "source": [
        "def construct_input_graph(data):\n",
        "  \"\"\"\n",
        "  Given a processed question, the function will return a tuple of torch tensors\n",
        "  in the format of the input of dgl.graph()\n",
        "\n",
        "  text: question's text\n",
        "  inputs: The list of input arguments of the function\n",
        "  input_to_text: The list of indices corresponding to the most related input.\n",
        "  \"\"\"\n",
        "  \n",
        "  adj_matrix = torch.eye(n , dtype=torch.bool)\n",
        "  for i, j in enumerate(data['dependency_parse']):\n",
        "    adj_matrix[i][j] = True\n",
        "\n",
        "  return torch.nonzero(adj_matrix, as_tuple=True)\n",
        "\n",
        "def tensorize_data(data):\n",
        "  \"\"\"\n",
        "  Build torch tensors that represent the dataset,\n",
        "  \"\"\"\n",
        "  for d in data:\n",
        "        # Indices of the in_tokens in the in_vocab\n",
        "        d['in_idxs'] = torch.tensor([in_vocab.token2idx.get(x, in_vocab.unk) for x in d['in_tokens']])\n",
        "        d['n_in'] = n_in = len(d['in_idxs'])\n",
        "        d['n_nP'] = n_nP = len(d['nP'])\n",
        "        # True if the position in the input has a quantity\n",
        "        d['nP_in_mask'] = mask = torch.zeros(n_in, dtype=torch.bool)\n",
        "        mask[d['nP_positions']] = True\n",
        "        if 'out_tokens' in d:\n",
        "            # Indices of the out_tokens in the out_vocab\n",
        "            d['out_idxs'] = torch.tensor([out_vocab.token2idx.get(x, out_vocab.unk) for x in d['out_tokens']])\n",
        "            d['n_out'] = len(d['out_idxs'])\n",
        "            # A mask where the first n_nP elements are True\n",
        "            d['nP_out_mask'] = mask = torch.zeros(n_max_inputs, dtype=torch.bool)\n",
        "            mask[:n_nP] = True\n",
        "        # Graph edges for constructing the DGL graph later\n",
        "        d['edges'] = construct_input_graph(d)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgN5HSnNChCd"
      },
      "source": [
        "# Encoder Architecture\n",
        "\n",
        "The next bits of code design the architecture of the encoder. First up are the modules for transformers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDCVmawkCum_"
      },
      "source": [
        "class TransformerAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Used in Transformer Block, implements the dot-product attention\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.qkv = nn.Linear(n_hid, n_head * (n_k * 2 + n_v))\n",
        "        self.out = nn.Linear(n_head * n_v, n_hid)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        n_batch, n_batch_max_in, n_hid = x.shape\n",
        "        q_k_v = self.qkv(x).view(n_batch, n_batch_max_in, n_head, 2 * n_k + n_v).transpose(1, 2)\n",
        "        q, k, v = q_k_v.split([n_k, n_k, n_v], dim=-1)\n",
        "\n",
        "        q = q.reshape(n_batch * n_head, n_batch_max_in, n_k)\n",
        "        k = k.reshape_as(q).transpose(1, 2)\n",
        "        qk = q.bmm(k) / np.sqrt(n_k)\n",
        "\n",
        "        if mask is not None:\n",
        "            qk = qk.view(n_batch, n_head, n_batch_max_in, n_batch_max_in).transpose(1, 2)\n",
        "            qk[~mask] = -np.inf\n",
        "            qk = qk.transpose(1, 2).view(n_batch * n_head, n_batch_max_in, n_batch_max_in)\n",
        "        qk = qk.softmax(dim=-1)\n",
        "        v = v.reshape(n_batch * n_head, n_batch_max_in, n_v)\n",
        "        qkv = qk.bmm(v).view(n_batch, n_head, n_batch_max_in, n_v).transpose(1, 2).reshape(n_batch, n_batch_max_in, n_head * n_v)\n",
        "        out = self.out(qkv)\n",
        "        return x + out\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Custom Transformer\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.attn = TransformerAttention()\n",
        "        n_inner = n_hid * 4\n",
        "        self.inner = nn.Sequential(\n",
        "            nn.Linear(n_hid, n_inner),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(n_inner, n_hid)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        x = x + self.attn(x, mask=mask)\n",
        "        return x + self.inner(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nu1U5r02C2n_"
      },
      "source": [
        "The next section is for the Graph Convolutional Networks:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20COijyaG7oy"
      },
      "source": [
        "class GCNBranch(nn.Module):\n",
        "    def __init__(self, n_hid_in, n_hid_out, dropout=0.3):\n",
        "        super().__init__()\n",
        "        \"\"\"\n",
        "        Define a branch of the graph convolution with\n",
        "        1. GraphConv from n_hid_in to n_hid_in\n",
        "        2. ReLU\n",
        "        3. Dropout\n",
        "        4. GraphConv from n_hid_in to n_hid_out\n",
        "        \n",
        "        Note: your should call dgl.nn.GraphConv with allow_zero_in_degree=True\n",
        "        \"\"\"\n",
        "        ### Your code here ###\n",
        "        self.gc1 = GraphConv(n_hid_in, n_hid_in, allow_zero_in_degree=True)\n",
        "        self.drelu = nn.Sequential(nn.ReLU(inplace=True), nn.Dropout(dropout))\n",
        "        self.gc2 = GraphConv(n_hid_in, n_hid_out, allow_zero_in_degree=True)\n",
        "\n",
        "    def forward(self, x, graph):\n",
        "        \"\"\"\n",
        "        Forward pass of your defined branch above\n",
        "        \"\"\"\n",
        "        ### Your code here ###\n",
        "        return self.gc2(graph, self.drelu(self.gc1(graph, x)))\n",
        "\n",
        "class GCN(nn.Module):\n",
        "    \"\"\"\n",
        "    A graph convolution network with multiple graph convolution branches\n",
        "    \"\"\"\n",
        "    def __init__(self, n_head=4, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.branches = nn.ModuleList(GCNBranch(n_hid, n_hid // n_head, dropout) for _ in range(n_head))\n",
        "\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(n_hid, n_hid),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(n_hid, n_hid)\n",
        "        )\n",
        "        self.layer_norm = nn.LayerNorm(n_hid)\n",
        "        self.n_head = n_head\n",
        "\n",
        "    def forward(self, h, d_graph):\n",
        "        x = h.reshape(-1, n_hid)\n",
        "        graphs = [d_graph for i in range(n_head)]\n",
        "        x = torch.cat([branch(x, g) for branch, g in zip(self.branches, graphs)], dim=-1).view_as(h)\n",
        "        x = h + self.layer_norm(x)\n",
        "        return x + self.feed_forward(x)\n",
        "\n",
        "class Gate(nn.Module):\n",
        "    \"\"\"\n",
        "    Activation gate used a few times in the TreeDecoder\n",
        "    \"\"\"\n",
        "    def __init__(self, n_in, n_out):\n",
        "        super(Gate, self).__init__()\n",
        "        self.t = nn.Linear(n_in, n_out)\n",
        "        self.s = nn.Linear(n_in, n_out)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.t(x).tanh() * self.s(x).sigmoid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJpn415RbUWw"
      },
      "source": [
        "# Decoder\n",
        "\n",
        "The next batch of code is for the tree-based decoder:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3h4cRn0Nb2zu"
      },
      "source": [
        "class TreeDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Defines parameters and methods for decoding into an expression. Used in train and predict\n",
        "    \"\"\"\n",
        "    def __init__(self, dropout=0.5):\n",
        "        super().__init__()\n",
        "        drop = nn.Dropout(dropout)\n",
        "        \n",
        "        self.constant_embeddings = nn.Parameter(torch.randn(1, out_vocab.n_constants, n_hid))\n",
        "\n",
        "        self.qp_gate = nn.Sequential(drop, Gate(n_hid, n_hid))\n",
        "        self.right = nn.Sequential(drop, Gate(3*n_hid, n_hid)) # Right(q_l, G_c, t_l)\n",
        "        self.hasmore = nn.Sequential(drop, Gate(3*n_hid, 2)) #HasMore (q_r', G_c, t_r')\n",
        "\n",
        "        self.attn_fc = nn.Sequential(drop,\n",
        "            nn.Linear(2 * n_hid, n_hid),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(n_hid, 1)\n",
        "        )\n",
        "        self.quant_fc = nn.Sequential(drop,\n",
        "            nn.Linear(n_hid * 3, n_hid),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(n_hid, 1, bias=False)\n",
        "        )\n",
        "        self.op_fc = nn.Sequential(drop, nn.Linear(n_hid * 2, out_vocab.n_ops))\n",
        "\n",
        "        self.op_embedding = nn.Embedding(out_vocab.n_ops + 1, n_hid, padding_idx=out_vocab.n_ops)\n",
        "        self.left = nn.Sequential(drop, Gate(3 * n_hid, n_hid)) #Left(q_p, G_c, y)\n",
        "        self.left_qp = nn.Sequential(drop, Gate(3 * n_hid, n_hid), self.qp_gate) #Left_qp(q_p, G_c, y)\n",
        "\n",
        "        self.subtree_gate = nn.Sequential(drop, Gate(3 * n_hid, n_hid))\n",
        "        self.predict_keyword = nn.Sequential(drop, Gate(3 * n_hid, n_hid))\n",
        "    \n",
        "    def attention(self, q, zbar, in_mask=None):\n",
        "        \"\"\"\n",
        "        Corresponds roughly to the GTS-Attention function defined by the paper\n",
        "        \"\"\"\n",
        "        attn_score = self.attn_fc(\n",
        "            torch.cat([q.unsqueeze(1).expand_as(zbar), zbar], dim=2)\n",
        "        ).squeeze(2)\n",
        "        if in_mask is not None:\n",
        "            attn_score[~in_mask] = -np.inf\n",
        "        attn = attn_score.softmax(dim=1)\n",
        "        return (attn.unsqueeze(1) @ zbar).squeeze(1) # (n_batch, n_hid)\n",
        "\n",
        "    def predict(self, qp_Gc, quant_embed, nP_out_mask=None):\n",
        "        \"\"\"\n",
        "        Corresponds roughly to the GTS-Predict functions defined by the paper\n",
        "        \"\"\"\n",
        "        quant_score = self.quant_fc(\n",
        "            torch.cat([qp_Gc.unsqueeze(1).expand(-1, quant_embed.size(1), -1), quant_embed], dim=2)\n",
        "        ).squeeze(2)\n",
        "        op_score = self.op_fc(qp_Gc)\n",
        "        pred_score = torch.cat((op_score, quant_score), dim=1)\n",
        "        if nP_out_mask is not None:\n",
        "            pred_score[:, out_vocab.base_nP:][~nP_out_mask] = -np.inf\n",
        "        return pred_score\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXLleMGhUM49"
      },
      "source": [
        "# Train Function\n",
        "\n",
        "When training, we train on the sequence of operations needed for the construction of the tree, which was made available through preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEl7W_kUURiS"
      },
      "source": [
        "class Node:\n",
        "    \"\"\"\n",
        "    Node for tree traversal during training\n",
        "    \"\"\"\n",
        "    def __init__(self, parent):\n",
        "        self.parent = parent\n",
        "        self.is_root = (parent is None)\n",
        "        self.children = []\n",
        "        self.cur_right = None\n",
        "        self.ql = None\n",
        "        self.tl = None\n",
        "        self.func = None\n",
        "        self.keyword = None\n",
        "\n",
        "def train(batch, model, opt):\n",
        "    \"\"\"\n",
        "    Computes the loss on a batch of inputs, and takes a step with the optimizer\n",
        "    \"\"\"\n",
        "    n_batch = len(batch)\n",
        "    n_in = [d['n_in'] for d in batch]\n",
        "    pad = lambda x, value: nn.utils.rnn.pad_sequence(x, batch_first=True, padding_value=value)\n",
        "    in_idxs = pad([d['in_idxs'] for d in batch], in_vocab.pad).to(device)\n",
        "    in_mask = pad([torch.ones(n, dtype=torch.bool) for n in n_in], False).to(device)\n",
        "\n",
        "    graph = dgl.batch([dgl.graph(d['edges'], num_nodes=in_idxs.size(1), device=device) for d in batch])\n",
        "\n",
        "    zbar, qroot = model.encode(in_idxs, n_in, graph, in_mask=None)\n",
        "    decoder = model.decoder\n",
        "\n",
        "    qp = decoder.qp_gate(qroot)\n",
        "\n",
        "    label = pad([d['out_idxs'] for d in batch], out_vocab.pad)\n",
        "\n",
        "    z_func = zbar.new_zeros((n_batch, n_max_inputs, n_hid))\n",
        "    # Takes the embeddings of the target words and places them in a z_func matrix\n",
        "    z_func[func_out_mask] = zbar[func_in_mask]\n",
        "\n",
        "    n_quant = out_vocab.n_constants + n_max_inputs\n",
        "    # Embeddings of all the input arguments + constants\n",
        "    quant_embed = torch.cat([decoder.constant_embedding.expand(n_batch, -1, -1), z_nP], dim=1) # (n_batch, n_quant, n_hid)\n",
        "\n",
        "    nodes = np.array([Node(None) for _ in range(n_batch)])\n",
        "    func_min, func_max = out_vocab.base_op, out_vocab.base_op + out_vocab.n_ops\n",
        "    quant_min, quant_max = out_vocab.base_quant, out_vocab.base_quant + n_quant\n",
        "\n",
        "    scores = []\n",
        "    prev_label = torch.zeros(n_batch)\n",
        "    for i, label_i in enumerate(label.T):\n",
        "        Gc = decoder.attention(qp, zbar, in_mask)\n",
        "        qp_Gc = torch.cat([qp, Gc], dim=1)\n",
        "        \n",
        "        # Fix so that this is constructed later\n",
        "        score = decoder.predict(qp_Gc, quant_embed, func_out_mask)\n",
        "        scores.append(score)\n",
        "\n",
        "        # Determine which function to send the input through next\n",
        "        to_send_left = (func_min <= prev_label) and (prev_label <= func_max)\n",
        "        to_send_hm = ((quant_min <= prev_label) and (prev_label <= quant_max)) or (0 == label_i)\n",
        "        to_send_hm_up = (0 == label_i)\n",
        "        to_send_key = ((func_min <= label_i) and (label_i <= func_max)) or ((quant_min <= label_i) and (label_i <= quant_max))\n",
        "        to_send_right = (1 == label_i)\n",
        "\n",
        "        # Deal with Left() case first\n",
        "        func_embed = decoder.op_embedding((prev_label[to_send_left] - out_vocab.base_op).to(device))\n",
        "        qp_Gc_func = torch.cat([qp_Gc[to_send_left], func_embed], dim=1)\n",
        "        qleft = decoder.left(qp_Gc_func)\n",
        "\n",
        "        for j, ql, func in zip(to_send_left.nonzero(as_tuple=True)[0], qleft, func_embed):\n",
        "            node = nodes[j]\n",
        "            nodes[j] = Node(node)\n",
        "            node.children.append(nodes[j])\n",
        "            node.cur_right = nodes[j]\n",
        "            node.func = func\n",
        "            node.ql = ql\n",
        "\n",
        "        # Next, with HasMore()\n",
        "        for j in to_send_hm_up.nonzero(as_tuple=True)[0]:\n",
        "            pnode = nodes[j].up\n",
        "            pnode.tl = decoder.merge_subtree(pnode.op, pnode.tl, nodes[j].ql) # Check !!\n",
        "            nodes[j] = pnode\n",
        "        \n",
        "        qr_prime = torch.stack([nodes[j].ql for j in to_send_hm.nonzero(as_tuple=True)])\n",
        "        tr_prime = torch.stack([nodes[j].tl for j in to_send_hm.nonzero(as_tuple=True)])\n",
        "        qr_Gc_tr_hm = torch.cat([qr_prime, Gc, tr_prime], dim=1)\n",
        "        hm_out = decoder.hasmore(qr_Gc_tr_hm)\n",
        "\n",
        "        # Next, with PredictKeyword()\n",
        "        q = torch.stack([nodes[j].ql for j in to_send_key.nonzero(as_tuple=True)])\n",
        "        y_hat = torch.stack([nodes[j].op for j in to_send_key.nonzero(as_tuple=True)])\n",
        "        q_Gc_yhat = torch.cat([q, Gc, y_hat], dim=1)\n",
        "        key_out = decoder.predict_keyword(q_Gc_yhat)\n",
        "\n",
        "        # Finally, with Right()\n",
        "        qr_prime = torch.stack([nodes[j].ql for j in to_send_right.nonzero(as_tuple=True)])\n",
        "        tr_prime = torch.stack([nodes[j].tl for j in to_send_right.nonzero(as_tuple=True)])\n",
        "        qr_Gc_tr_right = torch.cat([qr_prime, Gc, tr_prime], dim=1)\n",
        "        right_out = decoder.right(qr_Gc_tr_right)\n",
        "\n",
        "        for j, qr in zip(to_send_right.nonzero(as_tuple=True)[0], right_out):\n",
        "            node = nodes[j]\n",
        "            nodes[j] = Node(node)\n",
        "            node.cur_right = nodes[j]\n",
        "            nodes.children.append(nodes[j])\n",
        "            nodes.ql = qr \n",
        "\n",
        "        prev_label = label_i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h51D0S6HpCuD"
      },
      "source": [
        "# Predict Function\n",
        "\n",
        "The next function is used for prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gz0fHDItpMv6"
      },
      "source": [
        "class BeamNode(Node):\n",
        "    \"\"\"\n",
        "    Node for beam search during evaluation\n",
        "    \"\"\"\n",
        "    def __init__(self, up, prev, qp, token=None):\n",
        "        super().__init__(up)\n",
        "        self.prev = prev\n",
        "        self.qp = qp\n",
        "        self.token = token\n",
        "\n",
        "    def trace_tokens(self, *last_token):\n",
        "        if self.prev is None:\n",
        "            return list(last_token)\n",
        "        tokens = self.prev.trace_tokens()\n",
        "        tokens.append(self.token)\n",
        "        tokens.extend(last_token)\n",
        "        return tokens\n",
        "\n",
        "def predict(d, model, beam_size=5, n_max_out=45, mode='max_likelihood'):\n",
        "    \"\"\"\n",
        "    Predict the idxs corresponding to an expression given the inputs. Leverages beam search to maximize\n",
        "    prediction probability\n",
        "\n",
        "    d: The piece of data to predict\n",
        "    model: The trained model\n",
        "    beam_size: Size associated with beam search\n",
        "    n_max_out: Cap on the number of nodes in expression tree construction\n",
        "    mode: either 'max_likelihood' or 'sample'\n",
        "    \"\"\"\n",
        "    in_idxs = d['in_idxs'].unsqueeze(0).to(device=device)\n",
        "    graph = dgl.graph(d['edges'], num_nodes=in_idxs.size(1), device=device)\n",
        "    zbar, qroot = model.encode(in_idxs, [d['n_in']], graph, in_mask=None)\n",
        "    z_func = zbar[:, d['func_positions']]\n",
        "\n",
        "    decoder = model.decoder\n",
        "    quant_embed = torch.cat([model.constant_embeddings, z_func], dim=1)\n",
        "\n",
        "    func_min, func_max = out_vocab.base_op, out_vocab.base_op + out_vocab.n_ops\n",
        "    quant_min, quant_max = out_vocab.base_quant, out_vocab.base_quant + n_quants\n",
        "\n",
        "    if mode == 'max_likelihood':\n",
        "        best_done_beam = (-np.inf, None)\n",
        "        beams = [(0, BeamNode(up=None, prev=None, qp=decoder.qp_gate(qroot)))]\n",
        "        for _ in range(n_max_out):\n",
        "            new_beams = []\n",
        "            for logp_prev, node in beams:\n",
        "                Gc = decoder.attention(node.qp, zbar)\n",
        "                qp_Gc = torch.cat([node.qp, Gc], dim=1)\n",
        "                log_prob = decoder.predict(qp_Gc, quant_embed).log_softmax(dim=1)\n",
        "                top_logps, top_tokens = log_prob.topk(beam_size, dim=1)\n",
        "                for logp_token_, out_token_ in zip(top_logps.unbind(dim=1), top_tokens.unbind(dim=1)):\n",
        "                    out_token = out_token_.item()\n",
        "                    logp = logp_prev + logp_token_.item()\n",
        "                    if quant_min <= out_token <= quant_max:\n",
        "                        construct = False\n",
        "                        while not construct:\n",
        "                            qr_Gc_tr = torch.cat([node.qp, Gc, node.tl], dim=1)\n",
        "                            hm = decoder.hasmore(qr_Gc_tr)\n",
        "                            if hm.argmax(dim=1) == 1:\n",
        "                                construct = True\n",
        "                            else:\n",
        "                                node = node.up\n",
        "                    elif func_min <= out_token <= func_max:\n",
        "                        func_embed = decoder.op_embedding(out_token)\n",
        "                        qp_Gc_func = torch.cat([qp_Gc, func_embed], dim=1)\n",
        "                        prev_node = copy(node)\n",
        "                        next_node = prev_node.left = BeamNode(\n",
        "                            up=prev_node, prev=prev_node,\n",
        "                            qp=decoder.left_qp(qp_Gc_op)\n",
        "                            token=out_token\n",
        "                        )\n",
        "                        prev_node.op = op_embed\n",
        "                        prev_node.ql = decoder.left(qp_Gc_op)\n",
        "                    \n",
        "\n",
        "    elif mode == 'sample':\n",
        "        pass\n",
        "    else:\n",
        "        raise ValueError(\"Mode can only be max_likelihood or sample\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwuAHN-_SbyY"
      },
      "source": [
        "# Training Loop\n",
        "\n",
        "The next set of code is the actual training loop; running this cell will train the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kengGLpcA8cQ"
      },
      "source": [
        "use_t5 = 'small' # Value should be None, 'small', or 'base'\n",
        "model_save_dir = f'models/{use_t5 or \"custom\"}'\n",
        "os.makedirs(model_save_dir, exist_ok=True)\n",
        "\n",
        "# IMPORTANT NOTE: if you change some of these hyperparameters during training,\n",
        "# you will also need to change them during prediction (see next section)\n",
        "n_max_in = 100\n",
        "n_epochs = 100\n",
        "n_batch = 64\n",
        "learning_rate = 1e-3\n",
        "if use_t5:\n",
        "    # T5 hyperparameters\n",
        "    freeze_layers = []\n",
        "    weight_decay = 1e-5\n",
        "    n_hid = dict(small=512, base=768)[use_t5] # Do not modify unless you want to try t5-large\n",
        "    n_k = n_v = 64\n",
        "    n_head = 8\n",
        "else:\n",
        "    # Custom transformer hyperparameters\n",
        "    n_layers = 3\n",
        "    n_hid = 512\n",
        "    n_k = n_v = 64\n",
        "    n_head = 8\n",
        "    weight_decay = 0\n",
        "device = 'cuda:0'\n",
        "\n",
        "train_data, val_data, in_vocab, out_vocab, n_max_nP, t5_model = setup(use_t5)\n",
        "tensorize_data(itertools.chain(train_data, val_data))\n",
        "\n",
        "model = Model()\n",
        "opt = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(opt, n_epochs)\n",
        "model.to(device)\n",
        "\n",
        "epoch = 0\n",
        "i = 0\n",
        "while epoch < n_epochs:\n",
        "    print('Epoch:', epoch + 1)\n",
        "    model.train()\n",
        "    losses = []\n",
        "    for start in trange(0, len(train_data), n_batch):\n",
        "        batch = sorted(train_data[start: start + n_batch], key=lambda d: -d['n_in'])\n",
        "        loss = train(batch, model, opt, i)\n",
        "        losses.append(loss)\n",
        "        i += 1\n",
        "    scheduler.step()\n",
        "\n",
        "    print(f'Training loss: {np.mean(losses):.3g}')\n",
        "\n",
        "    epoch += 1\n",
        "    if epoch % 10 == 0:\n",
        "        model.eval()\n",
        "        value_match, equation_match = [], []\n",
        "        with torch.no_grad():\n",
        "            for d in tqdm(val_data):\n",
        "                pred = predict(d, model)\n",
        "                d['pred_tokens'] = [out_vocab.idx2token[idx] for idx in pred]\n",
        "                val_match, eq_match = check_match(pred, d)\n",
        "                value_match.append(val_match)\n",
        "                equation_match.append(eq_match)\n",
        "        print(f'Validation expression accuracy: {np.mean(equation_match):.3g}')\n",
        "        print(f'Validation value accuracy: {np.mean(value_match):.3g}')\n",
        "        # We save the model every 10 epochs, feel free to load in a trained model with\n",
        "        # model.load_state_dict(torch.load(f'models/model-{epoch}.pth'))\n",
        "        # Note: if you want to restart training from a saved model, you must also save and load the optimizer with\n",
        "        # torch.save(opt.state_dict(), os.path.join(model_save_dir, f'opt-{epoch}.pth'))\n",
        "        torch.save(model.state_dict(), os.path.join(model_save_dir, f'model-{epoch}.pth'))\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yv8Z9BLPHBNt"
      },
      "source": [
        "# Predictions\n",
        "\n",
        "The next batch of code allows multiple predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52umG8DzHI58"
      },
      "source": [
        "use_t5 = 'small'\n",
        "eval_epoch = 30\n",
        "device = 'cpu'\n",
        "\n",
        "# Make sure your parameter here is the exact same as the parameters you trained with,\n",
        "# else the model will not load correctly\n",
        "n_max_in = 100\n",
        "if use_t5:\n",
        "    # T5 hyperparameters\n",
        "    freeze_layers = []\n",
        "    n_hid = dict(small=512, base=768)[use_t5] # Do not modify unless you want to try t5-large\n",
        "    n_k = n_v = 64\n",
        "    n_head = 8\n",
        "else:\n",
        "    # Custom transformer hyperparameters\n",
        "    n_layers = 3\n",
        "    n_hid = 512\n",
        "    n_k = n_v = 64\n",
        "    n_head = 8\n",
        "\n",
        "test_data, in_vocab, out_vocab, n_max_nP, t5_model = setup(use_t5, do_eval=True)\n",
        "model = Model()\n",
        "model.load_state_dict(torch.load(f'models/{use_t5 or \"custom\"}/model-{eval_epoch}.pth'))\n",
        "tensorize_data(test_data)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for d in tqdm(test_data): # There's no quadratics in the test_data, fortunately\n",
        "        pred = predict(d, model)\n",
        "        d['pred_tokens'] = pred_tokens = [out_vocab.idx2token[idx] for idx in pred]\n",
        "        d['subbed_tokens'] = subbed_tokens = sub_nP(pred_tokens, d['nP'])\n",
        "        d['Predicted'] = round(evaluate_prefix_expression(subbed_tokens), 3) # Make sure to round to 3 decimals\n",
        "\n",
        "import pandas as pd\n",
        "predictions = pd.DataFrame(test_data).set_index('Id')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
